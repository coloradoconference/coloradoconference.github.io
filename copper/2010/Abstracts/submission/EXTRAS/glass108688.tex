\documentclass{report}
\usepackage{amsmath,amssymb}
\setlength{\parindent}{0mm}
\setlength{\parskip}{1em}
\begin{document}
\begin{center}
\rule{6in}{1pt} \
{\large Alan H. Glassser \\
{\bf A Scalable Parallel Extended MHD Solver: Application of Physics-Based Preconditioning to High-Order Spectral Elements}}

3833 NE 155th St \\ Lake Forest Park \\ WA 98155
\\
{\tt ahg5@u.washington.edu}\\
A. H. Glasser\\
V. S. Lukin\end{center}

\renewcommand*{\v}[1]{\hbox{\bfseries #1}}
\renewcommand*{\t}[1]{\hbox{\sffamily\bfseries #1}}

We describe an application of physics-based preconditioning \cite{Cha08}
to spectral element discretization. HiFi is a 2D and 3D nonlinear fluid
simulation code, written in Fortran 95 for efficient operation on
distributed-memory parallel computers, with emphasis on extended MHD and
magnetic fusion energy. The code is separated into a large solver
library and a much smaller application module which links to the
library, using flux-source form for the physics equations. A large
range of realistic nonlinear and time-dependent boundary conditions has
been developed.

Spatial discretization uses high-order $C^0$ spectral elements on a
curvilinear grid. Grid cells are logically rectangular, using Cartesian
product of 1D polynomial modal basis functions. Time discretization
uses a fully implicit Newton-Krylov method with adaptive time steps.

The greatest on computer time and storage is due to the need to solve
large, sparse, ill-conditioned linear systems, resulting from multiple
time scales. Static condensation is used to eliminate amplitudes of
higher-order spectral elements in terms of linear elements by small,
local direct solves, leaving reduced-order and better-conditioned
matrices, to be solved by distributed parallel iterative methods. HiFi
is built on the PETSc library \cite{petsc} for efficient parallel
operation and access to advanced methods of linear and nonlinear system
solution. Our goal is weakly scalable parallel solution as problem size
and number of parallel processors are scaled up.

Our approach is physics-based preconditioning \cite{Cha08}, in which the
physical dependent variables are partitioned into two sets as the basis
of further reducing the order and increasing the diagonal dominance.
For example, in visco-resistive MHD, set 1 consists of density,
pressure, magnetic flux function, and current, while set 2 consists of
the momentum densities, The linear system is expressed in block form as
\begin{equation}
\t{L} \v{u} = \v{r}, \quad
\t{L} \equiv \left( \begin{matrix} \t{L}_{11} & \t{L}_{12} \\
\t{L}_{21} & \t{L}_{22} \end{matrix} \right), \quad
\v{u} = \left( \begin{matrix} \v{u}_1 \\ \v{u}_2 \end{matrix} \right), \quad
\v{r} = \left( \begin{matrix} \v{r}_1 \\ \v{r}_2 \end{matrix} \right).
\end{equation}
Any matrix of this block form can be formally factored to give
\begin{equation}
\t{L} = \left( \begin{matrix} \t{I} & \t{0} \\
\t{L}_{21} \t{L}_{11}^{-1} & \t{I} \end{matrix} \right)
\left( \begin{matrix} \t{L}_{11} & \t{0} \\ \t{0} & \t{S} \end{matrix} \right)
\left( \begin{matrix} \t{I} & \t{L}_{11}^{-1} \t{L}_{12}
\\ \t{0} & \t{I} \end{matrix} \right),
\end{equation}
with Schur complement matrix defined by
\begin{equation}
\t{S} \equiv \t{L}_{22} - \t{L}_{21} \t{L}_{11}^{-1} \t{L}_{12}.
\label{eq:schur}
\end{equation}
After factorization, a formal inverse can be expressed as
\begin{equation}
\t{L}^{-1} = \left( \begin{matrix} \t{I} & - \t{L}_{11}^{-1} \t{L}_{12} \\
\t{0} & \t{I} \end{matrix} \right)
\left( \begin{matrix} \t{L}_{11}^{-1} & \t{0} \\ \t{0} & \t{S}^{-1}
\end{matrix} \right)
\left( \begin{matrix} \t{I} & \t{0} \\
- \t{L}_{21} \t{L}_{11}^{-1} & \t{I} \end{matrix} \right),
\end{equation}
in which only the solutions of the smaller block matrices $\t{L}_{11}$
and $\t{S}$ are required, thereby already simplifying solution. It is
then straightforward to solve the system:
\begin{equation}
\v{s}_1 = \t{L}_{11}^{-1} \v{r}_1, \quad
\v{s}_2 = \v{r}_2 - \t{L}_{21} \v{s}_1, \quad
\v{u}_2 = \t{S}^{-1} \v{s}_2, \quad
\v{u}_1 = \v{s}_1 - \t{L}_{11}^{-1} \t{L}_{12} \v{u}_2.
\label{eq:solution}
\end{equation}
Exact solution of $\t{S}$ is impractical because, while the $\t{L}_{ij}$
are sparse, the presense of $\t{L}_{11}^{-1}$ in Eq. (\ref{eq:schur})
makes $\t{S}$ dense.. We introduce an approximation $\t{P} \approx
\t{L}^{-1}$, use it as a preconditioner, and finish the solution with a
preconditioned Krylov iteration, $\left( \t{L} \t{P} \right) \left(
\t{P}^{-1} \v{u} \right) = \v{r}$. As long as $\t{P}$ constitutes a
sufficiently accurate approximate inverse, the preconditioned Krylov
iteration should converge rapidly, resulting in an effectively exact
solution of the full problem.

We approximate $\t{S}$ by reversing the order of discretization and
substition, giving it the form of the well-known ideal MHD force
operator, expressed in terms of the divergence of a stress tensor. This
leads to the approximate Schur complement
\begin{equation}
\t{S} \approx \t{L}_{22}
- h^2 \theta^2 \left< \nabla \cdot \t{T} \right>
\end{equation}
with $h$ the time step, $\theta$ the time-centering parameter, and
brackets representing spectral element discretization. The stress
tensor is given by
\begin{eqnarray}
\t{T} = \left( \v{B} \cdot \frac{\partial \v{B}}{\partial t}
+ \frac{\partial p}{\partial t} \right) \t{I}
- \v{B} \frac{\partial \v{B}}{\partial t}
- \frac{\partial \v{B}}{\partial t} \v{B} \nonumber \\
= \left[ \v{B} \cdot \nabla \times \left(\v{v} \times \v{B} \right)
- \gamma p \nabla \cdot \v{v} - \v{v} \cdot \nabla p \right] \t{I} \\
- \v{B} \nabla \times \left(\v{v} \times \v{B} \right)
- \nabla \times \left(\v{v} \times \v{B} \right) \v{B}. \nonumber
\end{eqnarray}
in the flux-source form, as required by our spectral element
discretization.

The matrices $\t{L}_{11}$ and $\t{S}$ have reduced order and are more
diagonally dominant than the full Jacobian. They are further reduced by
static condensation and then solved by GMRES, preconditioned by
additive-Schwarz blockwise LU. This is followed by Newton-Krylov
iteration on the full nonlinear system, using matrix-free GMRES. The
convergence rate is measured by the number of KSP iterations required
for the block solves, reflecting the condition number of the
preconditioning matrices; and by the number of Newton iterations,
reflecting the accuracy of the approximate Schur complement. Inaccuracy
influences the rate of convergence but not the final solution.

Future efforts will be devoted to weak scaling tests to determine the
limits of this approach on large parallel computers; exploration of
other methods of solution for the reduced preconditioning equations,
available through PETSc; and extending the physics content of the
approximate Schur complement to include two-fluid effects.

\bibliographystyle{unsrt}
\begin{thebibliography} {00}
\bibitem{Cha08}L. Chac\'on, L., An optimal, parallel, fully implicit
Newton-Krylov solver for three-dimensional visco-resistive
magnetohydrodynamic, Phys. Plasmas {\bf 15}, 056103 (2008).
\bibitem{petsc}Satish Balay, Kris Buschelman, William D. Gropp,
Dinesh Kaushik, Matt Knepley, Lois Curfman McInnes, Barry F. Smith, and
Hong Zhang, PETSc Users Manual, Technical Report ANL-95/11 Revision
3.0.0, Argonne National Laboratory, 2008.
\end{thebibliography}


\end{document}
