<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">

<!--Converted with LaTeX2HTML 2002-2-1 (1.70)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>detou090855</TITLE>
<META NAME="description" CONTENT="detou090855">
<META NAME="keywords" CONTENT="detou090855">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META NAME="Generator" CONTENT="LaTeX2HTML v2002-2-1">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="detou090855.css">

<LINK REL="next" HREF="node1.html">
</HEAD>

<BODY >
<!--Navigation Panel-->
<A NAME="tex2html2"
  HREF="node1.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="file:/usr/share/latex2html/icons/next.png"></A> 
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="file:/usr/share/latex2html/icons/up_g.png"> 
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="file:/usr/share/latex2html/icons/prev_g.png">   
<BR>
<B> Next:</B> <A NAME="tex2html3"
  HREF="node1.html">About this document ...</A>
<BR>
<BR>
<!--End of Navigation Panel-->
<DIV ALIGN="CENTER">
  <FONT SIZE="+1">Sylvie Detournay 
<BR><B>Multigrid methods for zero-sum two player stochastic games with mean reward</B></FONT>
</DIV>
<P>
<DIV ALIGN="CENTER">INRIA Saclay and CMAP 
<BR>
Ecole Polytechnique 
<BR>
Route de Saclay 
<BR>
91128 Palaiseau Cedex 
<BR>
France

<BR><TT>sylvie.detournay@inria.fr</TT>
<BR>
Marianne Akian
</DIV>

<P>
We develop a fast numerical algorithm for large scale zero-sum two player
stochastic games with perfect information and mean reward, which combines
policy iteration and algebraic multigrid methods.

<P>
Consider a finite state space <!-- MATH
 $\mathcal{X}$
 -->
<IMG
 WIDTH="18" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img1.png"
 ALT="$ \mathcal{X}$">. The stochastic game is played in
stages as follows. The initial state <IMG
 WIDTH="21" HEIGHT="30" ALIGN="MIDDLE" BORDER="0"
 SRC="img2.png"
 ALT="$ x_0$"> is given and known by the two
players. The player who plays first, says <SMALL>MAX</SMALL>, chooses an action in a
set of possible actions. Then the second player, called <SMALL>MIN</SMALL>, chooses an
action in another set of possible actions. The actions of both players
and the current state determine a payment made by <SMALL>MIN</SMALL> to <SMALL>MAX</SMALL> at stage
0 and the probability of the new state <IMG
 WIDTH="21" HEIGHT="30" ALIGN="MIDDLE" BORDER="0"
 SRC="img3.png"
 ALT="$ x_1$">. Then the game continue in
the same way with state <IMG
 WIDTH="21" HEIGHT="30" ALIGN="MIDDLE" BORDER="0"
 SRC="img3.png"
 ALT="$ x_1$"> and so on. We call a strategy or policy for
a player, a rule which tells him the action to choose in any situation. A
Markovian strategy depends only, possibly randomly, on the current state
and not on the past history or stage. Each pair of Markovian stationary
strategies of the two players determines a Markov chain on <!-- MATH
 $\mathcal{X}$
 -->
<IMG
 WIDTH="18" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img1.png"
 ALT="$ \mathcal{X}$">. We are
studying the value of the game with mean reward which is defined as the
mean expected payment per stage, made by <SMALL>MIN</SMALL> to <SMALL>MAX</SMALL>, when each player
chooses a strategy maximizing his reward.

<P>
The value of the game is solution of a dynamic programing equation. This
nonlinear equation can be solved by the policy iteration algorithm for
zero sum stochastic games of Hoffman and Karp (66) when the Markov
transition matrices of the game are all irreducible. The principle of
this algorithm consists in applying successively the two following steps:
first compute the value of the game with fixed strategy for the first
player and then improve this strategy. The first step is solved applying
the policy iteration for one player games, i.e. stochastic control
problems. Cochet-Terrasson and Gaubert (06) proposed a version of policy
iteration for two player games in the general multichain case, which is
based on the algorithm for multichain Markov decision processes of
Denardo and Fox (68). Each iteration of Dernado and Fox algorithm
requires the computation of stationary probabilities of irreducible
Markov chains and also the solution of linear systems of the type <!-- MATH
 $v = M
v + r$
 -->
<IMG
 WIDTH="87" HEIGHT="30" ALIGN="MIDDLE" BORDER="0"
 SRC="img4.png"
 ALT="$ v = M
v + r$"> where <IMG
 WIDTH="22" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img5.png"
 ALT="$ M$"> is a sub-markovian matrix.

<P>
We propose an algorithm based on Cochet-Terrasson and Gaubert policy
iteration algorithm where we use multigrid methods for Markov chains of
Horton (94) and De Sterck and all (08) to find the stationary
probabilities and algebraic multigrid algorithm of Ruge and St&#252;ben
(86) for the above linear systems.
We present numerical results of this algorithm (implemented in C) for
large scale zero-sum two player games.

<P>
<BR><HR>
<!--Table of Child-Links-->
<A NAME="CHILD_LINKS"></A>

<UL>
<LI><A NAME="tex2html4"
  HREF="node1.html">About this document ...</A>
</UL>
<!--End of Table of Child-Links-->
<HR>
<!--Navigation Panel-->
<A NAME="tex2html2"
  HREF="node1.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="file:/usr/share/latex2html/icons/next.png"></A> 
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="file:/usr/share/latex2html/icons/up_g.png"> 
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="file:/usr/share/latex2html/icons/prev_g.png">   
<BR>
<B> Next:</B> <A NAME="tex2html3"
  HREF="node1.html">About this document ...</A>
<!--End of Navigation Panel-->
<ADDRESS>
Copper Mountain Conference
2011-02-20
</ADDRESS>
</BODY>
</HTML>
